{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ltyt2yuv_vT"
      },
      "source": [
        "# 1. Stimulus Generation\n",
        "Generate deceptive and honest explanations using OpenAI's GPT-3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxgb2XeUrga9"
      },
      "outputs": [],
      "source": [
        "# download packages\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install openai\n",
        "!pip uninstall pandas\n",
        "!pip install pandas==1.1.5\n",
        "!pip install language-tool-python\n",
        "\n",
        "# restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# load grammar check\n",
        "import language_tool_python\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# load spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "tzXMQAc0_Qg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IYr-5wKXr3F"
      },
      "outputs": [],
      "source": [
        "# load the original news headlines\n",
        "df = pd.read_csv(\"news_items_raw.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpdU-AZMdndA"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2zqErxFWI7N"
      },
      "outputs": [],
      "source": [
        "# OpenAI key\n",
        "OPENAI_API_KEY = \"\" # <-- insert your own API key\n",
        "\n",
        "# Load your API key from an environment variable or secret management service\n",
        "openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYrvuO7GO93U"
      },
      "outputs": [],
      "source": [
        "# The prompt for each generated explanation\n",
        "\n",
        "# download liarplus dataset\n",
        "!git clone https://github.com/Tariq60/LIAR-PLUS.git\n",
        "\n",
        "# load liarplus dataset\n",
        "liarplus_train_file = \"LIAR-PLUS/dataset/jsonl/train2.jsonl\"\n",
        "data = []\n",
        "with open(liarplus_train_file) as f:\n",
        "  for line in f:\n",
        "    data.append(json.loads(line))\n",
        "\n",
        "liar_df = pd.DataFrame(data)\n",
        "liar_df = liar_df.drop('id',1)\n",
        "liar_df = liar_df.drop('json_file_id',1)\n",
        "liar_df = liar_df.drop('topics',1)\n",
        "liar_df = liar_df.drop('originator',1)\n",
        "liar_df = liar_df.drop('party',1)\n",
        "\n",
        "train = liar_df[liar_df['label'] != 'half-true']\n",
        "train = train[train['label'] != 'mostly-true']\n",
        "train = train[train['label'] != 'barely-true']\n",
        "train = train[train['label'] != 'half-false']\n",
        "train = train[train['label'] != 'pants-fire']\n",
        "train = train[train['title'] == '']\n",
        "train = train.drop('title',1)\n",
        "\n",
        "# randomly select 12 items\n",
        "sample_size = 12\n",
        "\n",
        "finetune = \"\"\n",
        "for n, i in enumerate(random.sample(list(train.index.values), sample_size)):\n",
        "  veracity = 'TRUE' if train.at[i, 'label'] == 'REFUTES' else 'FALSE'\n",
        "  explanation = train.at [i, 'justification'].replace('\"', '').replace(\"'\",\"\")\n",
        "  finetune += f\"claim: \\\"{train.at[i, 'claim']}\\\",\\nlabel: \\\"{veracity}\\\",\\nexplanation: The claim is {veracity} because {explanation[0].lower() + explanation[1:]}###\\n\"\n",
        "\n",
        "print(finetune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFaDXeddw4GY"
      },
      "outputs": [],
      "source": [
        "### Generate Explanations for each item based on the prompt\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "  for n, veracity in enumerate([\"TRUE\", \"FALSE\"]):\n",
        "    prompt_ = f'{finetune}claim:\\\"{df[\"argument\"].iloc[i]}\\\",\\nlabel: \\\"{veracity}\\\",\\nexplanation: The claim is {veracity} because'\n",
        "    column = [\"honest\" if veracity == df[\"ground truth\"].iloc[i] else \"lie\"][0]\n",
        "    df[column+\" veracity\"].iloc[i] = veracity\n",
        "\n",
        "    for j in range(3):\n",
        "      ncolumn = column + str(j)\n",
        "\n",
        "      try:\n",
        "        if df[\"argument\"].iloc[i] != \"\" and df.at[i+1, ncolumn] == \"\":\n",
        "          response = openai.Completion.create(engine=\"davinci\", prompt=prompt_, max_tokens=50, temperature=1, stop=\"###\")[\"choices\"][0][\"text\"]\n",
        "          df.at[i+1, ncolumn] = response\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOjT12vo4zwR"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfW1ovVr9h5Y"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"stimuli_raw_news.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X01fLV-lieN"
      },
      "source": [
        "#2. Stimulus Ranking\n",
        "Stimuli were then ranked by their repeated word frequency and semantic similarity to the headline/trivia item, and correct grammar of the AI generated explanations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uEdTAnTNTxpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load fact and logic checked dataset\n",
        "df = pd.read_csv(\"stimuli_raw_news.csv\")"
      ],
      "metadata": {
        "id": "VvDDwbryTy-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check grammar, semantic similarity, and word frequency\n",
        "def cleanResponse(claim, rsp):\n",
        "\n",
        "  # check semantic sim\n",
        "  doc1 = nlp(claim)\n",
        "  doc2 = nlp(rsp)\n",
        "  sim = doc1.similarity(doc2)\n",
        "\n",
        "  # count grammar errors\n",
        "  matches = tool.check(rsp)\n",
        "  gram = len(matches)\n",
        "\n",
        "  # count repeated words\n",
        "  words = [w for w in rsp.split(\" \")]\n",
        "  freq = np.mean([words.count(c) for c in claim.split(\" \")])\n",
        "\n",
        "  stats = {'sim'      : sim,\n",
        "           'gram_err' : gram,\n",
        "           'freq'     : freq\n",
        "           }\n",
        "\n",
        "  return stats"
      ],
      "metadata": {
        "id": "uZlO50m688L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-P5n_4ko8m"
      },
      "outputs": [],
      "source": [
        "# Run stats on stimuli\n",
        "for i in tqdm(range(len(df))):\n",
        "  for j in range(5):\n",
        "    for veracity in ['lie','honest']:\n",
        "      claim = df.at[i+1, \"argument\"]\n",
        "      explanation = df.at[i+1, veracity+str(j)]\n",
        "      stats = cleanResponse(claim, explanation)\n",
        "      df.at[i+1, veracity+str(j)+'_similarity'] = stats[\"sim\"]\n",
        "      df.at[i+1, veracity+str(j)+'_gramma_err'] = stats[\"gram_err\"]\n",
        "      df.at[i+1, veracity+str(j)+'_word_freq'] = stats[\"freq\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEAmC79JODcP"
      },
      "outputs": [],
      "source": [
        "# Select best stimuli\n",
        "df_sorted = df.copy()\n",
        "\n",
        "k=1\n",
        "for i in range(len(df)):\n",
        "  for veracity in ['lie','honest']:\n",
        "    \n",
        "    sample = pd.DataFrame()\n",
        "    sim_best = {\"val\": 0,\n",
        "              \"vrc\": \"\",\n",
        "              \"arg\": \"\",\n",
        "              \"exp\": \"\"}\n",
        "              \n",
        "    for j in range(3):\n",
        "      sample.at[j, 'arg'] = df.at[i+1, \"argument\"]\n",
        "      sample.at[j, 'exp'] = f\"This is {df.at[i+1,veracity+' veracity']}. {df.at[i+1, veracity+str(j)]}\"\n",
        "      sample.at[j, 'val'] = df.at[i+1, veracity+str(j)+'_similarity'] \n",
        "      sample.at[j, 'vrc'] = veracity\n",
        "\n",
        "      if df.at[i+1, veracity+str(j)+'_similarity'] > sim_best[\"val\"]:\n",
        "        sim_best[\"val\"] = df.at[i+1, veracity+str(j)+'_similarity'] \n",
        "        sim_best[\"vrc\"] = veracity\n",
        "        sim_best[\"arg\"] = df.at[i+1, \"argument\"]\n",
        "        sim_best[\"exp\"] = df.at[i+1, veracity+str(j)]\n",
        "\n",
        "    sample = sample.sort_values(by=['val'], ascending=False)\n",
        "    sample = sample.reset_index(drop=True)\n",
        "\n",
        "    for j in range(len(sample)):\n",
        "      df_sorted.at[k, veracity+str(j)] = sample.at[j, 'exp']\n",
        "\n",
        "  k+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5NxVqT1-Tqv"
      },
      "outputs": [],
      "source": [
        "## Clean the dataset\n",
        "\n",
        "# remove NaN\n",
        "df_sorted = df_sorted.fillna(\"\")\n",
        "\n",
        "# remove unfinished sentences\n",
        "for veracity in ['lie', 'honest']:\n",
        "  cols = [veracity+str(i) for i in range(3)]\n",
        "  for col in cols:\n",
        "    df_sorted[col] = df_sorted[col].apply(lambda x: x.rpartition('.')[0])\n",
        "\n",
        "  df_sorted[veracity] = df_sorted[cols].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "  df_sorted = df_sorted.drop(cols, 1)\n",
        "\n",
        "# remove cols\n",
        "for col in [\"Unnamed: 0\", \"Unnamed: 9\", \"veracity\"]:\n",
        "  try:\n",
        "    df_sorted = df_sorted.drop(col, 1)\n",
        "  except:\n",
        "    print(f\"{col} not found. Skipping...\")\n",
        "\n",
        "# remove empty items\n",
        "honest = df_fact_checked[df_fact_checked['honest'] != \"\"]\n",
        "lie = df_fact_checked[df_fact_checked['lie'] != \"\"]\n",
        "df_sorted = honest.append(lie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFFJb5ZDL4L_"
      },
      "outputs": [],
      "source": [
        "df_sorted.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQpfQXtNLqtI"
      },
      "outputs": [],
      "source": [
        "df_sorted.to_csv(\"stimuli_sorted_news.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fact and logic checking\n",
        "\n",
        "Next, the stimuli are downloaded and manually checked for factual accuracy and logical validity. The criteria for **factual accuracy** were as follows: \"A statement is true if and only if a credible news source within the first 2 pages of a google search states it to be so\". The criteria for **logical validity** were as follows: \"If the explanation can be true and the headline/trivia item false\". A stimulus is rated as \"unsubstantiated\" if the explanation repeats the content of the headline/trivia item without providing a reason. After the fact and logic, we then exclude the logically invalid, contradicting, and unsubstantiated items."
      ],
      "metadata": {
        "id": "YRbM9vo-L7vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import sample"
      ],
      "metadata": {
        "id": "U-Lvhr2rPdBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load fact and logic checked dataset\n",
        "df = pd.read_csv(\"stimuli_checked_news.csv\")"
      ],
      "metadata": {
        "id": "2gi5MsmTN4D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lKgzK_nLlX1"
      },
      "outputs": [],
      "source": [
        "# remove unsubstantiated and contradictory statements\n",
        "for veracity in ['lie', 'honest']:\n",
        "  df = df[df[f'{veracity} logical validity'] != '']\n",
        "  df = df[df[f'{veracity} logical validity'] != 'Circular']\n",
        "  df = df[df[f'{veracity} veracity'] != '']\n",
        "  df = df[df[f'{veracity} veracity'] != 'Self-contradiction']\n",
        "  df = df[df[f'{veracity} veracity'] != 'Unsubstantiated']\n",
        "\n",
        "\n",
        "# remove true lies and false honesties\n",
        "df = df[df['honest veracity'] != 'FALSE']\n",
        "df = df[df['lie veracity'] != 'TRUE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orc0b2ZsbXIm"
      },
      "outputs": [],
      "source": [
        "# Equalize the number of valid and invalid stimuli across honest and deceptive conditions\n",
        "\n",
        "# deceptive\n",
        "df_lie_valid = df[df['lie logical validity'] == 'VALID']\n",
        "df_lie_invalid = df[df['lie logical validity'] == 'INVALID']\n",
        "\n",
        "min = df[f'lie logical validity'].value_counts().min()\n",
        "df_lie_valid = df_valid.sample(min)\n",
        "df_lie_invalid = df_invalid.sample(min)\n",
        "\n",
        "df = df_lie_valid.append(df_lie_invalid)\n",
        "\n",
        "# honest\n",
        "df_honest_valid = df[df['honest logical validity'] == 'VALID']\n",
        "df_honest_invalid = df[df['honest logical validity'] == 'INVALID']\n",
        "\n",
        "min = df[f'honest logical validity'].value_counts().min()\n",
        "df_honest_valid = df_honest_valid.sample(min)\n",
        "df_honest_invalid = df_honest_invalid.sample(min)\n",
        "\n",
        "df = df_honest_valid.append(df_honest_invalid)\n",
        "\n",
        "# Print results\n",
        "print(f\"Deceptive Condition: Distribution of Logical Validity:\\n{df['lie logical validity'].value_counts()}\\n\")\n",
        "print(f\"Honest Condition: Distribution of Logical Validity:\\n{df['honest logical validity'].value_counts()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esqRB3lt-7mD"
      },
      "outputs": [],
      "source": [
        "# Save processed dataset as final\n",
        "df.to_csv(\"stimuli_processed_news.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9RQnzHbpZld"
      },
      "source": [
        "# Extra: Run stats on stimulus set\n",
        "Check linguistic differences between lie and honest conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07PvqNIu6c79"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "!pip install textstat\n",
        "\n",
        "# load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "import textstat\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# load models\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L54MUL94xolf"
      },
      "outputs": [],
      "source": [
        "# load processed dataset\n",
        "df = pd.read_csv(\"stimuli_processed_news.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQPYKL9E6SPc"
      },
      "outputs": [],
      "source": [
        "# drop irrelevant columns\n",
        "df_mc = df.drop(['stimuli_ID','argument', 'ground truth', 'lie veracity','lie logical validity','honest veracity', 'honest logical validity', 'lie_flag', 'honest_flag'],1)\n",
        "df_mc = pd.melt(df_mc, var_name=\"veracity\", value_name=\"explanation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aI6CISK1n4v"
      },
      "outputs": [],
      "source": [
        "# Plot functions\n",
        "\n",
        "def t_test(data, ivar, dvar):\n",
        "  \"\"\"Run t-test on two columns in dataset\"\"\"\n",
        "\n",
        "  ivar_vals = [vals for vals in data[ivar].unique()]\n",
        "  if len(ivar_vals) != 2:\n",
        "    raise Exception('Error: Dependent variables more or less than 2!')\n",
        "  \n",
        "  grp_0 = data[data[ivar]==ivar_vals[0]][dvar]\n",
        "  grp_1 = data[data[ivar]==ivar_vals[1]][dvar]\n",
        "\n",
        "  t2, p2 = stats.ttest_ind(grp_0,grp_1)\n",
        "\n",
        "  return f\"t={t2}, p={p2}, \", [f\"{ivar_vals[i]}_mean = {key.mean()}, {ivar_vals[i]}_std = {np.std(key)}\" for i, key in enumerate([grp_0, grp_1])]\n",
        "\n",
        "\n",
        "\n",
        "def plot_num(df, ivar, dvar, ylim):\n",
        "  \"\"\"Plot numeric columns in dataset\"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=[4,8])\n",
        "  \n",
        "  sns.barplot(x=df[ivar], y=df[dvar], ax=ax, \n",
        "              #palette=sns.diverging_palette(15, 160, n=2)) \n",
        "              palette = ['crimson','lightskyblue'])\n",
        "  ax.set_title(dvar)\n",
        "  ax.set_ylim(ylim) if ylim != [None] else \"\"\n",
        "  plt.show()\n",
        "\n",
        "  print(t_test(df, ivar, dvar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSXOdp0aAqqQ"
      },
      "outputs": [],
      "source": [
        "# Compute text stats\n",
        "stats = pd.DataFrame()\n",
        "\n",
        "for key in ['lie','honest']:\n",
        "  df_mc['Word Count'] = [len(tokenizer.tokenize(df_mc.at[i, \"explanation\"])) for i in range(len(df_mc))] # word count\n",
        "  df_mc['Sentiment (TextBlob)'] = [TextBlob(df_mc.at[i, \"explanation\"]).sentiment.polarity for i in range(len(df_mc))] # sentiment using TextBlob\n",
        "  df_mc['Sentiment (Vader)'] = [np.argmax(list(sid.polarity_scores(df_mc.at[i, \"explanation\"]).values())[:-1]) for i in range(len(df_mc))] # sentiment using Vader\n",
        "  df_mc['Subjectivity'] = [TextBlob(df_mc.at[i, \"explanation\"]).sentiment.subjectivity for i in range(len(df_mc))] # opinioniated\n",
        "  df_mc['Reading Ease'] = [textstat.flesch_reading_ease(df_mc.at[i, \"explanation\"]) for i in range(len(df_mc))] # reading ease\n",
        "  df_mc['Grade Level'] = [textstat.flesch_kincaid_grade(df_mc.at[i, \"explanation\"]) for i in range(len(df_mc))] # grade level\n",
        "\n",
        "# Plot text stats\n",
        "columns = {'Word Count':          [None] ,\n",
        "           'Sentiment (TextBlob)':[-1, 1], \n",
        "           'Sentiment (Vader)':   [0,  2],\n",
        "           'Reading Ease':        [0,100],\n",
        "           'Subjectivity':        [0,  1],\n",
        "           'Grade Level':         [None] }\n",
        "\n",
        "[plot_num(df_mc, \"veracity\", key, columns[key]) for key in columns]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}