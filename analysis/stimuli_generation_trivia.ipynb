{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ltyt2yuv_vT"
      },
      "source": [
        "# 1. Stimulus Generation\n",
        "Generate deceptive and honest explanations using OpenAI's GPT-3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxgb2XeUrga9"
      },
      "outputs": [],
      "source": [
        "# download packages\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install openai\n",
        "!pip uninstall pandas\n",
        "!pip install pandas==1.1.5\n",
        "!pip install language-tool-python\n",
        "\n",
        "# restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# load grammar check\n",
        "import language_tool_python\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# load spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "tzXMQAc0_Qg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IYr-5wKXr3F"
      },
      "outputs": [],
      "source": [
        "# load fact and logic checked dataset\n",
        "df = pd.read_csv(\"trivia_items_raw.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpdU-AZMdndA"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2zqErxFWI7N"
      },
      "outputs": [],
      "source": [
        "# OpenAI key\n",
        "OPENAI_API_KEY = \"\" # <-- insert your own API key\n",
        "\n",
        "# Load your API key from an environment variable or secret management service\n",
        "openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYrvuO7GO93U"
      },
      "outputs": [],
      "source": [
        "# The prompt for each generated explanation\n",
        "\n",
        "finetune = \"\"\"\n",
        "###\n",
        "claim: \"Jordan North won I'm A Celebrity... Get Me Out Of Here! in 2020\"\n",
        "label: \"FALSE\"\n",
        "explanation: The claim is FALSE because he came runner up to Giovanna Fletcher.\n",
        "###\n",
        "claim: An octopus has three hearts\n",
        "label: \"TRUE\",\n",
        "explanation: \"The claim is TRUE because an octopus has one main, systemic heart to pumps blood around its body. The two additional hearts are responsible for pumping blood over each of its gills.\"\n",
        "###\n",
        "claim: \"Thomas Edison discovered gravity\"\n",
        "label: \"FALSE\"\n",
        "explanation: The claim is FALSE because Isaac Newton discovered Gravity when he saw a falling apple while thinking about the forces of nature. Thomas Edison is thought to have invented the lightbulb..\n",
        "###\n",
        "claim: \"Walt Disney holds the record for the most Oscars\"\n",
        "label: \"TRUE\"\n",
        "Explanation: The claim is TRUE because Walt Disney won 26 Academy Awards (four of which were honorary) and was nominated a grand total of 59 times throughout his career\n",
        "###\n",
        "claim: \"Spaghetto is the singular word for a piece of spaghetti\"\n",
        "label: \"TRUE\"\n",
        "explanation: The claim is TRUE because generally, Italian words ending in \"o\" are singular and words ending in \"i\" are plural. Lots of spaghetto makes spaghetti.\n",
        "###\n",
        "claim: \"Venus is the hottest planet in the solar system\"\n",
        "label: \"TRUE\"\n",
        "explanation: The claim is TRUE because Venus has an average surface temperature of around 450Â° C. Mercury is closer to the sun, but has no atmosphere to regulate temperature it has a very large temperature fluctuation.\n",
        "###\n",
        "claim: \"Bananas are curved because they grow upwards towards the sun\"\n",
        "label: \"TRUE\"\n",
        "explanation: The claim is FALSE because a process called 'negative geotropism' means the fruit grows upwards to break through the canopy.\n",
        "###\n",
        "claim: \"Goldfish have a two second memory\"\n",
        "label: \"FALSE\"\n",
        "explanation: The claim is FALSE because their memories can actually last for months.\n",
        "###\n",
        "claim: \"There are 14 bones in a human foot\"\n",
        "label: \"FALSE\"\n",
        "explanation: The claim is FALSE because there are 28 bones in each foot.\n",
        "###\n",
        "claim: \"50 Cent and Charlie Chaplin were alive at the same time\"\n",
        "label: \"TRUE\"\n",
        "explanation: The claim is TRUE because Chaplin died when 50 Cent was about two and a half years old.\n",
        "###\n",
        "claim: \"Hot and cold water sound the same when poured\"\n",
        "label: \"FALSE\"\n",
        "explanation: The claim is FALSE because hot and cold water sound different due to the fact that hot water has a higher viscosity than cold water.\n",
        "###\n",
        "claim: \"The small intestine is about three-and-a-half times the length of your body\"\n",
        "label: \"TRUE\"\n",
        "explanation: The claim is TRUE because the small intestine is the longest section of your digestive tube, measuring about 22 feet (or seven meters) on average, or three-and-a-half times the length of your body\n",
        "###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFaDXeddw4GY"
      },
      "outputs": [],
      "source": [
        "### Generate Explanations for each items based on the prompt\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "  for n, veracity in enumerate([\"TRUE\", \"FALSE\"]):\n",
        "    prompt_ = f'{finetune}claim:\\\"{df[\"argument\"].iloc[i]}\\\",\\nlabel: \\\"{veracity}\\\",\\nexplanation: The claim is {veracity} because'\n",
        "    column = [\"honest\" if veracity == df[\"ground truth\"].iloc[i] else \"lie\"][0]\n",
        "    df[column+\" veracity\"].iloc[i] = veracity\n",
        "\n",
        "    for j in range(3):\n",
        "      ncolumn = column + str(j)\n",
        "\n",
        "      try:\n",
        "        if df[\"argument\"].iloc[i] != \"\" and df.at[i+1, ncolumn] == \"\":\n",
        "          response = openai.Completion.create(engine=\"davinci\", prompt=prompt_, max_tokens=50, temperature=1, stop=\"###\")[\"choices\"][0][\"text\"]\n",
        "          df.at[i+1, ncolumn] = response\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOjT12vo4zwR"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfW1ovVr9h5Y"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"stimuli_raw_trivia.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X01fLV-lieN"
      },
      "source": [
        "#2. Stimulus Ranking\n",
        "Stimuli were then ranked by their repeated word frequency and semantic similarity to the headline/trivia item, and correct grammar of the AI generated explanations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uEdTAnTNTxpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load fact and logic checked dataset\n",
        "df = pd.read_csv(\"stimuli_raw_trivia.csv\")"
      ],
      "metadata": {
        "id": "VvDDwbryTy-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check grammar, semantic similarity, and word frequency\n",
        "def cleanResponse(claim, rsp):\n",
        "\n",
        "  # check semantic sim\n",
        "  doc1 = nlp(claim)\n",
        "  doc2 = nlp(rsp)\n",
        "  sim = doc1.similarity(doc2)\n",
        "\n",
        "  # count grammar errors\n",
        "  matches = tool.check(rsp)\n",
        "  gram = len(matches)\n",
        "\n",
        "  # count repeated words\n",
        "  words = [w for w in rsp.split(\" \")]\n",
        "  freq = np.mean([words.count(c) for c in claim.split(\" \")])\n",
        "\n",
        "  stats = {'sim'      : sim,\n",
        "           'gram_err' : gram,\n",
        "           'freq'     : freq\n",
        "           }\n",
        "\n",
        "  return stats"
      ],
      "metadata": {
        "id": "uZlO50m688L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf-P5n_4ko8m"
      },
      "outputs": [],
      "source": [
        "# Run stats on stimuli\n",
        "for i in tqdm(range(len(df))):\n",
        "  for j in range(5):\n",
        "    for veracity in ['lie','honest']:\n",
        "      claim = df.at[i+1, \"argument\"]\n",
        "      explanation = df.at[i+1, veracity+str(j)]\n",
        "      stats = cleanResponse(claim, explanation)\n",
        "      df.at[i+1, veracity+str(j)+'_similarity'] = stats[\"sim\"]\n",
        "      df.at[i+1, veracity+str(j)+'_gramma_err'] = stats[\"gram_err\"]\n",
        "      df.at[i+1, veracity+str(j)+'_word_freq'] = stats[\"freq\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEAmC79JODcP"
      },
      "outputs": [],
      "source": [
        "# Select best stimuli\n",
        "df_sorted = df.copy()\n",
        "\n",
        "k=1\n",
        "for i in range(len(df)):\n",
        "  for veracity in ['lie','honest']:\n",
        "    \n",
        "    sample = pd.DataFrame()\n",
        "    sim_best = {\"val\": 0,\n",
        "              \"vrc\": \"\",\n",
        "              \"arg\": \"\",\n",
        "              \"exp\": \"\"}\n",
        "              \n",
        "    for j in range(3):\n",
        "      sample.at[j, 'arg'] = df.at[i+1, \"argument\"]\n",
        "      sample.at[j, 'exp'] = f\"This is {df.at[i+1,veracity+' veracity']}. {df.at[i+1, veracity+str(j)]}\"\n",
        "      sample.at[j, 'val'] = df.at[i+1, veracity+str(j)+'_similarity'] \n",
        "      sample.at[j, 'vrc'] = veracity\n",
        "\n",
        "      if df.at[i+1, veracity+str(j)+'_similarity'] > sim_best[\"val\"]:\n",
        "        sim_best[\"val\"] = df.at[i+1, veracity+str(j)+'_similarity'] \n",
        "        sim_best[\"vrc\"] = veracity\n",
        "        sim_best[\"arg\"] = df.at[i+1, \"argument\"]\n",
        "        sim_best[\"exp\"] = df.at[i+1, veracity+str(j)]\n",
        "\n",
        "    sample = sample.sort_values(by=['val'], ascending=False)\n",
        "    sample = sample.reset_index(drop=True)\n",
        "\n",
        "    for j in range(len(sample)):\n",
        "      df_sorted.at[k, veracity+str(j)] = sample.at[j, 'exp']\n",
        "\n",
        "  k+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5NxVqT1-Tqv"
      },
      "outputs": [],
      "source": [
        "## Clean the dataset\n",
        "\n",
        "# remove NaN\n",
        "df_sorted = df_sorted.fillna(\"\")\n",
        "\n",
        "# remove unfinished sentences\n",
        "for veracity in ['lie', 'honest']:\n",
        "  cols = [veracity+str(i) for i in range(3)]\n",
        "  for col in cols:\n",
        "    df_sorted[col] = df_sorted[col].apply(lambda x: x.rpartition('.')[0])\n",
        "\n",
        "  df_sorted[veracity] = df_sorted[cols].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
        "  df_sorted = df_sorted.drop(cols, 1)\n",
        "\n",
        "# remove cols\n",
        "for col in [\"Unnamed: 0\", \"Unnamed: 9\", \"veracity\"]:\n",
        "  try:\n",
        "    df_sorted = df_sorted.drop(col, 1)\n",
        "  except:\n",
        "    print(f\"{col} not found. Skipping...\")\n",
        "\n",
        "# remove empty items\n",
        "honest = df_fact_checked[df_fact_checked['honest'] != \"\"]\n",
        "lie = df_fact_checked[df_fact_checked['lie'] != \"\"]\n",
        "df_sorted = honest.append(lie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFFJb5ZDL4L_"
      },
      "outputs": [],
      "source": [
        "df_sorted.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQpfQXtNLqtI"
      },
      "outputs": [],
      "source": [
        "df_sorted.to_csv(\"stimuli_sorted_trivia.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fact and logic checking\n",
        "\n",
        "Next, the stimuli are downloaded and manually checked for factual accuracy and logical validity. The criteria for **factual accuracy** were as follows: \"A statement is true if and only if a credible news source within the first 2 pages of a google search states it to be so\". The criteria for **logical validity** were as follows: \"If the explanation can be true and the headline/trivia item false\". A stimulus is rated as \"unsubstantiated\" if the explanation repeats the content of the headline/trivia item without providing a reason. After the fact and logic, we then exclude the logically invalid, contradicting, and unsubstantiated items."
      ],
      "metadata": {
        "id": "YRbM9vo-L7vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import sample"
      ],
      "metadata": {
        "id": "U-Lvhr2rPdBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load fact and logic checked dataset\n",
        "df = pd.read_csv(\"stimuli_checked_trivia.csv\")"
      ],
      "metadata": {
        "id": "2gi5MsmTN4D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lKgzK_nLlX1"
      },
      "outputs": [],
      "source": [
        "# remove unsubstantiated and contradictory statements\n",
        "for veracity in ['lie', 'honest']:\n",
        "  df = df[df[f'{veracity} logical validity'] != '']\n",
        "  df = df[df[f'{veracity} logical validity'] != 'Circular']\n",
        "  df = df[df[f'{veracity} veracity'] != '']\n",
        "  df = df[df[f'{veracity} veracity'] != 'Self-contradiction']\n",
        "  df = df[df[f'{veracity} veracity'] != 'Unsubstantiated']\n",
        "\n",
        "\n",
        "# remove true lies and false honesties\n",
        "df = df[df['honest veracity'] != 'FALSE']\n",
        "df = df[df['lie veracity'] != 'TRUE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orc0b2ZsbXIm"
      },
      "outputs": [],
      "source": [
        "# Equalize the number of valid and invalid stimuli across honest and deceptive conditions\n",
        "\n",
        "# deceptive\n",
        "df_lie_valid = df[df['lie logical validity'] == 'VALID']\n",
        "df_lie_invalid = df[df['lie logical validity'] == 'INVALID']\n",
        "\n",
        "min = df[f'lie logical validity'].value_counts().min()\n",
        "df_lie_valid = df_valid.sample(min)\n",
        "df_lie_invalid = df_invalid.sample(min)\n",
        "\n",
        "df = df_lie_valid.append(df_lie_invalid)\n",
        "\n",
        "# honest\n",
        "df_honest_valid = df[df['honest logical validity'] == 'VALID']\n",
        "df_honest_invalid = df[df['honest logical validity'] == 'INVALID']\n",
        "\n",
        "min = df[f'honest logical validity'].value_counts().min()\n",
        "df_honest_valid = df_honest_valid.sample(min)\n",
        "df_honest_invalid = df_honest_invalid.sample(min)\n",
        "\n",
        "df = df_honest_valid.append(df_honest_invalid)\n",
        "\n",
        "# Print results\n",
        "print(f\"Deceptive Condition: Distribution of Logical Validity:\\n{df['lie logical validity'].value_counts()}\\n\")\n",
        "print(f\"Honest Condition: Distribution of Logical Validity:\\n{df['honest logical validity'].value_counts()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esqRB3lt-7mD"
      },
      "outputs": [],
      "source": [
        "# Save processed dataset as final\n",
        "df.to_csv(\"stimuli_processed_trivia.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9RQnzHbpZld"
      },
      "source": [
        "# Extra: Run stats on stimulus set\n",
        "Check linguistic differences between lie and honest conditions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07PvqNIu6c79"
      },
      "outputs": [],
      "source": [
        "# install packages\n",
        "!pip install textstat\n",
        "\n",
        "# load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "import textstat\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# load models\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L54MUL94xolf"
      },
      "outputs": [],
      "source": [
        "# load processed dataset\n",
        "df = pd.read_csv(\"stimuli_processed_trivia.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQPYKL9E6SPc"
      },
      "outputs": [],
      "source": [
        "# drop irrelevant columns\n",
        "df_mc = df.drop(['stimuli_ID','argument', 'ground truth', 'lie veracity','lie logical validity','honest veracity', 'honest logical validity', 'lie_flag', 'honest_flag'],1)\n",
        "df_mc = pd.melt(df_mc, var_name=\"veracity\", value_name=\"explanation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aI6CISK1n4v"
      },
      "outputs": [],
      "source": [
        "# Plot functions\n",
        "\n",
        "def t_test(data, ivar, dvar):\n",
        "  \"\"\"Run t-test on two columns in dataset\"\"\"\n",
        "\n",
        "  ivar_vals = [vals for vals in data[ivar].unique()]\n",
        "  if len(ivar_vals) != 2:\n",
        "    raise Exception('Error: Dependent variables more or less than 2!')\n",
        "  \n",
        "  grp_0 = data[data[ivar]==ivar_vals[0]][dvar]\n",
        "  grp_1 = data[data[ivar]==ivar_vals[1]][dvar]\n",
        "\n",
        "  t2, p2 = stats.ttest_ind(grp_0,grp_1)\n",
        "\n",
        "  return f\"t={t2}, p={p2}, \", [f\"{ivar_vals[i]}_mean = {key.mean()}, {ivar_vals[i]}_std = {np.std(key)}\" for i, key in enumerate([grp_0, grp_1])]\n",
        "\n",
        "\n",
        "\n",
        "def plot_num(df, ivar, dvar, ylim):\n",
        "  \"\"\"Plot numeric columns in dataset\"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=[4,8])\n",
        "  \n",
        "  sns.barplot(x=df[ivar], y=df[dvar], ax=ax, \n",
        "              #palette=sns.diverging_palette(15, 160, n=2)) \n",
        "              palette = ['crimson','lightskyblue'])\n",
        "  ax.set_title(dvar)\n",
        "  ax.set_ylim(ylim) if ylim != [None] else \"\"\n",
        "  plt.show()\n",
        "\n",
        "  print(t_test(df, ivar, dvar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSXOdp0aAqqQ"
      },
      "outputs": [],
      "source": [
        "# Compute text stats\n",
        "stats = pd.DataFrame()\n",
        "\n",
        "for key in ['lie','honest']:\n",
        "  df_mc['Word Count'] = [len(tokenizer.tokenize(df_mc.at[i, \"explanation\"])) for i in range(len(df_mc))] # word count\n",
        "  df_mc['Sentiment (TextBlob)'] = [TextBlob(df_mc.at[i, \"explanation\"]).sentiment.polarity for i in range(len(df_mc))] # sentiment using TextBlob\n",
        "  df_mc['Sentiment (Vader)'] = [np.argmax(list(sid.polarity_scores(df_mc.at[i, \"explanation\"]).values())[:-1]) for i in range(len(df_mc))] # sentiment using Vader\n",
        "  df_mc['Subjectivity'] = [TextBlob(df_mc.at[i, \"explanation\"]).sentiment.subjectivity for i in range(len(df_mc))] # opinioniated\n",
        "  df_mc['Reading Ease'] = [textstat.flesch_reading_ease(df_mc.at[i, \"explanation\"]) for i in range(len(df_mc))] # reading ease\n",
        "  df_mc['Grade Level'] = [textstat.flesch_kincaid_grade(df_mc.at[i, \"explanation\"]) for i in range(len(df_mc))] # grade level\n",
        "\n",
        "# Plot text stats\n",
        "columns = {'Word Count':          [None] ,\n",
        "           'Sentiment (TextBlob)':[-1, 1], \n",
        "           'Sentiment (Vader)':   [0,  2],\n",
        "           'Reading Ease':        [0,100],\n",
        "           'Subjectivity':        [0,  1],\n",
        "           'Grade Level':         [None] }\n",
        "\n",
        "[plot_num(df_mc, \"veracity\", key, columns[key]) for key in columns]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6ltyt2yuv_vT",
        "T9RQnzHbpZld",
        "kg2eD2OVtozP"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}