{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg2eD2OVtozP"
      },
      "source": [
        "# Data Post-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8QmjpVBptoID"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC5nAiH8tuJa"
      },
      "outputs": [],
      "source": [
        "# Load results\n",
        "df = pd.read_csv(\"raw_results.csv\")\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv(\"metadata.csv\", index_col='stimulus_ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juxHWLOJuF57"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gddBj4YAyUVO"
      },
      "outputs": [],
      "source": [
        "# Make column name include stimuli_ID\n",
        "new_df = df.copy()\n",
        "new_columns = []\n",
        "\n",
        "for column in df.columns:\n",
        "  if \"_initial_rating\" in column:\n",
        "    new_columns.append(f\"{str(''.join(filter(str.isdigit, df.at[0, column])))}_initial_rating\")\n",
        "  elif \"_second_rating\" in column:\n",
        "    new_columns.append(f\"{str(''.join(filter(str.isdigit, df.at[0, column])))}_second_rating\")\n",
        "  elif \"_prior_knowledge\" in column:\n",
        "    new_columns.append(f\"{str(''.join(filter(str.isdigit, df.at[0, column])))}_prior_knowledge\")\n",
        "  elif \"_page_1_time_Page\" in column:\n",
        "    new_columns.append(f\"{str(''.join(filter(str.isdigit, df.at[0, column])))}_page_1_time\")\n",
        "  elif \"_page_2_time_Page\" in column:\n",
        "    new_columns.append(f\"{str(''.join(filter(str.isdigit, df.at[0, column])))}_page_2_time\")\n",
        "  else:\n",
        "    new_columns.append(column)\n",
        "\n",
        "new_df.columns = new_columns\n",
        "new_df.drop([0,1], inplace=True)\n",
        "new_df = new_df.reset_index().drop(columns=['index'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count stimuli\n",
        "stimuli_n = sum(df.columns.str.count('_explanation_type'))\n",
        "\n",
        "# Drop nan\n",
        "new_df = new_df[new_df['S20_stimulus_ID'].notna()]\n",
        "\n",
        "# Make column name include stimuli number\n",
        "for i in tqdm(new_df.index):\n",
        "  stimuli_id = [int(new_df.at[i, f\"S{x}_stimulus_ID\"]) for x in range(1, stimuli_n+1)]\n",
        "\n",
        "  for n, j in enumerate(stimuli_id):\n",
        "    new_df.loc[i, f\"S{n+1}_initial_rating\"] = new_df.at[i, f'{j}_initial_rating']\n",
        "    new_df.loc[i, f\"S{n+1}_second_rating\"] = new_df.at[i, f'{j}_second_rating']\n",
        "    new_df.loc[i, f\"S{n+1}_knowledge\"] = new_df.at[i, f'7{j}_prior_knowledge']\n",
        "    new_df.loc[i, f\"S{n+1}_page_1_time\"] = new_df.at[i, f'{j}_page_1_time']\n",
        "    new_df.loc[i, f\"S{n+1}_page_2_time\"] = new_df.at[i, f'{j}_page_2_time']"
      ],
      "metadata": {
        "id": "MBa-EysnTd2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace string choices with numeric values\n",
        "\n",
        "\n",
        "# Cognitive Reflection Test to [0, 1] <-- 0 is incorrect, 1 is correct\n",
        "new_df.CRT_Q1 = pd.to_numeric(new_df.CRT_Q1, errors='coerce')\n",
        "new_df.CRT_Q2 = pd.to_numeric(new_df.CRT_Q2, errors='coerce')\n",
        "new_df.CRT_Q3 = pd.to_numeric(new_df.CRT_Q3, errors='coerce')\n",
        "\n",
        "new_df.CRT_Q1 = new_df.apply(lambda x: 1 if x.CRT_Q1 == 4 else 0, axis=1)  # ground truth is  4\n",
        "new_df.CRT_Q2 = new_df.apply(lambda x: 1 if x.CRT_Q2 == 29 else 0, axis=1) # ground truth is 29\n",
        "new_df.CRT_Q3 = new_df.apply(lambda x: 1 if x.CRT_Q3 == 20 else 0, axis=1) # ground truth is 20\n",
        "\n",
        "# Need for Cognition to [1,2,3,4,5]\n",
        "replacer = {'Strongly disagree': 1, 'Disagree': 2, 'Neither agree nor disagree': 3, 'Agree': 4, 'Strongly agree': 5}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "\n",
        "# Trust 00 to [1,2,3,4,5,6]\n",
        "replacer = {'Extremely Unreliable': 1, 'Moderately Unreliable': 2, 'Slightly Unreliable': 3, 'Slightly Reliable': 4, 'Moderately Reliable': 5,'Extremely Reliable' : 6}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "# Trust 01\n",
        "replacer = {'1-Not at all' : 1,'6-Very much so':6}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "# Trust 02\n",
        "replacer = {'Extremely NOT Misleading': 1,'Moderately NOT Misleading': 2,'Slightly NOT Misleading': 3,'Slightly Misleading': 4,'Moderately Misleading': 5,'Extremely Misleading' : 6}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "# Trust 03\n",
        "replacer = {'Extremely Inaccurate': 1, 'Moderately Inaccurate': 2,'Slightly Inaccurate': 3,'Slightly accurate': 4,'Moderately accurate': 5,'Extremely accurate' : 6}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "# Trust 04\n",
        "replacer = {'1-Not at all acting in my interest': 1,'6-Very much acting in my interest' : 6}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "# Trust 05\n",
        "replacer = {'Extremely Fair': 1,'Moderately Fair': 2,'Slightly Fair': 3,'Slightly Unfair': 4,'Moderately Unfair': 5,'Extremely Unfair': 6}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "\n",
        "# Attention checks to [Pass, Fail]\n",
        "replacer = {'Somewhat disagree' : 'Pass','Red,Green': 'Pass','FoxNews.com,NBC.com': 'Pass','New York Times website,Huffington Post,CNN.com,FoxNews.com,Google News':'Pass','Huffington Post,CNN.com,FoxNews.com':'Pass','CNN.com,FoxNews.com,Google News':'Pass'}\n",
        "cols = new_df.columns[new_df.dtypes == 'object']\n",
        "new_df[cols] = new_df[cols].replace(replacer)\n",
        "\n",
        "# Make non-pass attention checks into fail\n",
        "new_df.loc[~new_df.ps_0.isin(['Pass']), 'ps_0'] = 'Fail'\n",
        "new_df.loc[~new_df.ps_1.isin(['Pass']), 'ps_1'] = 'Fail'\n",
        "new_df.loc[~new_df.att1.isin(['Pass']), 'att1'] = 'Fail'\n",
        "new_df.loc[~new_df.att2.isin(['Pass']), 'att2'] = 'Fail'"
      ],
      "metadata": {
        "id": "HTS9YplXDF3b"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute aggregate and weighted scores\n",
        "trust_n = sum(new_df.columns.str.count('trust_'))\n",
        "NFC_n = sum(new_df.columns.str.count('NFC_Q'))\n",
        "CRT_n = sum(new_df.columns.str.count('CRT_Q'))\n",
        "\n",
        "for i in new_df.index:\n",
        "  new_df.at[i, 'trust'] = np.mean([float(new_df.at[i, f'trust_0{j}']) for j in range(trust_n)])\n",
        "  new_df.at[i, 'NFC'] = np.mean([new_df.at[i, f'NFC_Q{j}'] for j in range(1, NFC_n+1)])\n",
        "  new_df.at[i, 'CRT'] = np.mean([new_df.at[i, f'CRT_Q{j}'] for j in range(1, CRT_n+1)])"
      ],
      "metadata": {
        "id": "xWqvXYJuCdPU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Unfinished Entries\n",
        "print(f\"Unfinished entries: {len(new_df[new_df['Finished'] == 'False'])}\")\n",
        "new_df = new_df[new_df[\"Finished\"] != \"False\"]"
      ],
      "metadata": {
        "id": "Dq7LnjiICm6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itgi2mjsbyaW"
      },
      "outputs": [],
      "source": [
        "# Remove participants that failed an attention check\n",
        "new_df = new_df[(new_df.ps_1 == 'Pass') & (new_df.att1 == 'Pass') | (new_df.att1 == 'Pass') & (new_df.att2 == 'Pass') | (new_df.ps_1 == 'Pass') & (new_df.att2 == 'Pass')]\n",
        "\n",
        "print(f\"Participants before attention checks: {df.PROLIFIC_PID.nunique()}\")\n",
        "print(f\"Participants after attention checks: {new_df.PROLIFIC_PID.nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melt ratings to individual rows\n",
        "\n",
        "dfs = []\n",
        "for rating in ['initial_rating', 'second_rating', 'knowledge', 'page_1_time', 'page_2_time','stimulus_ID', 'explanation_type']:\n",
        "  \n",
        "  columns = [f\"S{i}_{rating}\" for i in range(1, stimuli_n+1)]\n",
        "  rating_df = pd.melt(new_df, id_vars =['PROLIFIC_PID'], value_vars = columns, var_name ='order', value_name=rating)\n",
        "  rating_df['order'] = rating_df['order'].str.replace(r'page_1',\"\").str.replace(r'page_2',\"\").str.replace(r'\\D', '')\n",
        "  dfs.append(rating_df)\n",
        "\n",
        "# merge\n",
        "merged = dfs[0]\n",
        "for i in dfs[1:]:\n",
        "  merged = merged.merge(i, how='left', on=['PROLIFIC_PID', 'order'])\n",
        "\n",
        "# add other columns\n",
        "keep = ['PROLIFIC_PID', 'RecordedDate','Duration (in seconds)','Comments', 'trust', 'NFC', 'CRT', 'condition', 'stimulation']\n",
        "processed_df = merged.merge(new_df[keep], how='left', on='PROLIFIC_PID')"
      ],
      "metadata": {
        "id": "MQyQRAQGW1tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge df and metadata\n",
        "processed_df['stimulus_ID'] = pd.to_numeric(processed_df['stimulus_ID'])\n",
        "\n",
        "processed_df['ground_truth'] = processed_df.apply(lambda x: metadata.loc[int(x['stimulus_ID']), 'ground_truth'], axis=1)\n",
        "processed_df['logical_validity'] = processed_df.apply(lambda x: metadata.loc[int(x['stimulus_ID']), x['explanation_type'].lower()+'_logical_validity'], axis=1)\n",
        "processed_df['topic'] = processed_df.apply(lambda x: metadata.loc[int(x['stimulus_ID']), 'topic'], axis=1)\n"
      ],
      "metadata": {
        "id": "JsUsApfHEfMJ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_df.head(3)"
      ],
      "metadata": {
        "id": "qMp6y_UUR0Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save processed df\n",
        "processed_df.to_csv('results_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "9YR3Jy0nAnQs"
      },
      "execution_count": 70,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6ltyt2yuv_vT",
        "T9RQnzHbpZld",
        "kg2eD2OVtozP"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
